{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba28d1f2-a390-40fc-9bde-830f3ab7bbd2",
   "metadata": {},
   "source": [
    "## 작업 마무리 후 확인사항 (필독)\n",
    "- autoviz 시각화를 보여주는 최대 row, col 확인하기!!!!!!(유의사항 언급 필요.) (完)\n",
    "- 그룹을 크게 잡은 작업은 캡션이 필요해보임. 내부에서 캡션 달아놓기! 근데...아웃풋을 최소화한다면 이작업이 필요는 없지 않을까...(output 삭제 유지) (完)\n",
    "- adult_prepro 함수 한줄 요약처리하자. 전처리 기능을 포함하고 있다고 했으니.(完)\n",
    "- 모델 학습에서 lightgbm과 모델 시각화 중 shape value는 상당히 많은 시간을 소요하는데, 실제 고객에게 보여주기 전까지는 각주 해제 금지\n",
    "- compare_model부분에서 모델을 자유롭게 선택할 수 있도록 유동적으로 개발이 필요?(完)\n",
    "- (input을 활용하여 전역변수를 입력받을 수 있게 처리)\n",
    "- (백엔드 파일 코드별로 설명하는 각주 작성.)\n",
    "- (모델 편향 before,after 반영해서 수정이 필요하다. lighthouse-copy1참조)\n",
    "- (회귀모델에 관한 지표도 추가해야해.)\n",
    "- (api url 입력 전역변수화 필요.)\n",
    "- (pygwalker 파일 html로 입력 받아 수집데이터 메타정보에 포함.)\n",
    "- 시스템 편향 shap value - waterfall (기본 전처리 실행.)\n",
    "- 운영 이력관리 의사결정 로그, 학습데이터 변경이력 json 형태로 넣는 코드 추가 필요.\n",
    "- api url 유동적으로 입력받을 수 있게 수정이 필요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1913b72e-362c-418f-a9ae-ad0f3b568d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported v0.1.905. Please call AutoViz in this sequence:\n",
      "    AV = AutoViz_Class()\n",
      "    %matplotlib inline\n",
      "    dfte = AV.AutoViz(filename, sep=',', depVar='', dfte=None, header=0, verbose=1, lowess=False,\n",
      "               chart_format='svg',max_rows_analyzed=150000,max_cols_analyzed=30, save_plot_dir=None)\n"
     ]
    }
   ],
   "source": [
    "# lighthouse_func.ipynb\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "from autoviz import data_cleaning_suggestions\n",
    "from sklearn.metrics import accuracy_score, precision_score,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity,EqualizedOdds, BoundedGroupLoss\n",
    "import warnings\n",
    "import pygwalker as pyg\n",
    "import importlib\n",
    "import import_ipynb\n",
    "from datetime import date\n",
    "from fairlearn.metrics import *\n",
    "from contextlib import redirect_stdout\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "global selected_model\n",
    "\n",
    "# 모델 학습에 pycaret기능을 사용하였는데, 이례적으로 pycaret 라이브러리는 분석 문제에 따라 임포트를 달리해줘야만 합니다.\n",
    "def lighthouse_setup(project_type):\n",
    "    \"\"\"\n",
    "    사용자로부터 문제 유형을 입력받아 적절한 PyCaret 모듈을 동적으로 임포트합니다.\n",
    "    \"\"\"\n",
    "    # project_type = input(\"문제 유형을 입력하세요 (분류 또는 회귀): \").lower()\n",
    "\n",
    "    if project_type == \"분류\":\n",
    "        module_name = \"classification\"\n",
    "    elif project_type == \"회귀\":\n",
    "        module_name = \"regression\"\n",
    "    else:\n",
    "        raise ValueError(\"잘못된 문제 유형입니다. '분류' 또는 '회귀'를 입력하세요.\")\n",
    "\n",
    "    pycaret_module = importlib.import_module(f\"pycaret.{module_name}\")\n",
    "    globals().update(pycaret_module.__dict__)\n",
    "\n",
    "    print(f\"pycaret.{module_name} 모듈이 성공적으로 import 되었습니다.\")\n",
    "\n",
    "# json db저장 코드\n",
    "def json_api(pid,table_name,json_data):\n",
    "    json_url = 'http://192.168.10.127:8000/postdb/light/insertFile/json/'\n",
    "    json_postVal = {\n",
    "            'val1':pid, # 프로젝트 ID\n",
    "            'val2':table_name, # json code?\n",
    "            'val3':json_data # json 데이터 or 텍스트로 변환된 이미지\n",
    "    }\n",
    "    response = requests.post(json_url,json = json_postVal)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # JSON 형식의 응답 데이터를 파이썬 딕셔너리로 변환\n",
    "        print(f\"Json id {data} 로 저장 완료됨\")\n",
    "    else:\n",
    "        print(f'오류: {response.status_code}')\n",
    "\n",
    "\n",
    "# pycaret으로 생성한 성능지표 이미지 db 저장 코드\n",
    "def pycaret_image_upload(pid, project_type,directory,dataset):\n",
    "    if not os.path.exists(directory):\n",
    "        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n",
    "\n",
    "    image_url = 'http://192.168.10.127:8000/postdb/light/insertFile/image/'\n",
    "    \n",
    "    # 테이블 이름과 해당하는 이미지 파일 이름을 매핑\n",
    "    if project_type == '분류':\n",
    "        table_image_map = {\n",
    "            \"AUC\": f\"{dataset}_AUC.png\",\n",
    "            \"precision Recall\": f\"{dataset}_Precision Recall.png\",\n",
    "            \"Confusion Matrix\": f\"{dataset}_Confusion Matrix.png\",\n",
    "            \"Threshold\": f\"{dataset}_Threshold.png\",\n",
    "            \"Feature_Importance\" : f\"{dataset}_Feature Importance.png\",\n",
    "            \"SHAP summary\" :  f\"{dataset}_SHAP summary.png\"\n",
    "        }\n",
    "    elif project_type == '회귀':\n",
    "        table_image_map = {\n",
    "            \"Residuals\" :  f\"{dataset}_Residuals.png\",\n",
    "            \"Prediction Error\" : f\"{dataset}_Prediction Error.png\",\n",
    "            \"Cooks Distance\" : f\"{dataset}_Cooks Distance.png\",\n",
    "            \"Learning Curve\" :  f\"{dataset}_Learning Curve.png\",\n",
    "            \"Feature Importance.\" : f\"{dataset}_Feature Importance.png\"\n",
    "            }\n",
    "    else :\n",
    "        print('분석문제를 다시 확인바랍니다')\n",
    "    \n",
    "    for table_name, image_file in table_image_map.items():\n",
    "        img_path = os.path.join(directory, image_file)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"경고: {img_path} 경로가 존재하지 않습니다.\")\n",
    "            continue\n",
    "        \n",
    "        # 이미지 파일 읽기 및 인코딩\n",
    "        try:\n",
    "            with open(img_path, 'rb') as img_file:\n",
    "                image_bytes = img_file.read()\n",
    "                encoded_string = base64.b64encode(image_bytes).decode('utf-8')\n",
    "            \n",
    "            print(f\"{image_file}의 인코딩 텍스트:\")\n",
    "            print(encoded_string[:20] + \"...\")  # 인코딩된 문자열의 일부만 출력\n",
    "            print()  # 각 이미지 출력 사이에 빈 줄 추가\n",
    "            \n",
    "            # API 요청 데이터 준비\n",
    "            image_postVal = {\n",
    "                'val1': pid,  # 프로젝트 ID\n",
    "                'val2': table_name,  # 테이블명\n",
    "                'val3': encoded_string  # 텍스트로 변환된 이미지\n",
    "            }\n",
    "            \n",
    "            # API 요청 보내기\n",
    "            response = requests.post(image_url, json=image_postVal)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()  # JSON 형식의 응답 데이터를 파이썬 딕셔너리로 변환\n",
    "                print(f\"{table_name} 저장 완료했습니다: {data}\")\n",
    "                print(\"\\n\",\"-\"*100)\n",
    "            else:\n",
    "                print(f'{table_name} 오류가 발생했습니다: {response.status_code}')\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"오류:  {img_path} 파일이 존재하지 않습니다\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"{table_name}으로의 api 요청이 실패했습니다: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{image_file}을 저장하는데 예기치못한 오류가 발생했습니다.: {e}\")\n",
    "\n",
    "# pycaret으로 추출한 성능지표를 json으로 변환하는 코드\n",
    "def convert_pycaretfair_to_json(project_type,pycaret_df, dataset_name):\n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    json_data = []\n",
    "    if project_type == '분류':\n",
    "        for _, row in pycaret_df.iterrows():\n",
    "            json_data.append({\n",
    "               \"Model\" : row[\"Model\"],\n",
    "                \"Accuracy\" : row[\"Accuracy\"],\n",
    "                \"AUC\" : row[\"AUC\"],\n",
    "                \"Recall\" : row[\"Recall\"],\n",
    "                \"Prec.\":row[\"Prec.\"] ,\n",
    "                \"F1\" : row[\"F1\"],\n",
    "                \"Kappa\" :row[ \"Kappa\"] ,\n",
    "                \"MCC\" :row[\"MCC\"] \n",
    "            })\n",
    "        # Convert to JSON string\n",
    "        json_string = json.dumps(json_data, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Write to file\n",
    "        with open(f'json/{dataset_name}_fairness.json', 'w') as file:\n",
    "            file.write(json_string)\n",
    "        \n",
    "        print(f\"파일명 json/{dataset_name}_fairness.json으로 json 저장 완료\")\n",
    "        # Optionally, return the JSON string\n",
    "        return json_string\n",
    "    elif project_type == '회귀':\n",
    "        for _, row in pycaret_df.iterrows():\n",
    "            json_data.append({\n",
    "               \"Model\" : row[\"Model\"],\n",
    "                \"MAE\" : row[\"MAE\"],\n",
    "                \"MSE\" : row[\"MSE\"],\n",
    "                \"RMSE\" : row[\"RMSE\"],\n",
    "                \"R2\" :row[\"R2\"] ,\n",
    "                \"RMSLE\" : row[\"RMSLE\"],\n",
    "                \"MAPE\" :row[ \"MAPE\"]\n",
    "            })\n",
    "        # Convert to JSON string\n",
    "        json_string = json.dumps(json_data, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Write to file\n",
    "        with open(f'json/{dataset_name}_fairness.json', 'w') as file:\n",
    "            file.write(json_string)\n",
    "        \n",
    "        print(f\"파일명 json/{dataset_name}_fairness.json으로 json 저장 완료\")\n",
    "        # Optionally, return the JSON string\n",
    "        return json_string\n",
    "\n",
    "# fairlearn으로 생성한 공정성 데이터프레임을 json으로 변환하는 코드\n",
    "def convert_fairlearn_to_json(fairlearn_df, dataset_name):\n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    json_data = []\n",
    "    for _, row in fairlearn_df.iterrows():\n",
    "        json_data.append({\n",
    "            \"date\": row['date'],\n",
    "            \"col_nm\": row['col_nm'],\n",
    "            \"demographic_parity_difference\": row['demographic_parity_difference'],\n",
    "            \"demographic_parity_ratio\": row['demographic_parity_ratio'],\n",
    "            \"equalized_odds_difference\": row['equalized_odds_difference'],\n",
    "            \"equalized_odds_ratio\": row['equalized_odds_ratio']\n",
    "        })\n",
    "    \n",
    "    # Convert to JSON string\n",
    "    json_string = json.dumps(json_data, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Write to file\n",
    "    with open(f'json/{dataset_name}_fairlearn.json', 'w') as file:\n",
    "        file.write(json_string)\n",
    "    \n",
    "    print(f\"파일명 json/{dataset_name}_fairlearn.json으로 json 저장 완료\")\n",
    "    \n",
    "    # Optionally, return the JSON string\n",
    "    return json_string\n",
    "\n",
    "# 데이터셋을 불러오는 코드\n",
    "def load_project(dataset,verboose):\n",
    "    \"\"\"\n",
    "    주어진 이름에 따라 데이터를 로드합니다.\n",
    "    \"\"\"\n",
    "    file_path = f'~/ori_data/{dataset}.csv'\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    print(f\"데이터셋 '{dataset}'이 성공적으로 로드되었습니다.\")\n",
    "    if verboose == True:\n",
    "        return df\n",
    "    else:\n",
    "        None\n",
    "\n",
    "# 원천 데이터셋에 대한 시각화.\n",
    "def lh_ori_vis(dataset,prediction):\n",
    "    df = pd.read_csv(f'~/ori_data/{dataset}.csv', encoding='utf-8')\n",
    "    print(\"\\n원천 데이터 시각화 1\")\n",
    "    walke1 = pyg.walk(df)\n",
    "    with open(f\"html/{dataset}_before.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pyg.to_html(df))\n",
    "    print(\"-\"*100)\n",
    "    print(\"\\n원천 데이터 시각화 2\")\n",
    "    # bias_before\n",
    "    %matplotlib inline\n",
    "    AV = AutoViz_Class()\n",
    "    # 시각화 결과 저장 경로\n",
    "    save_plot_dir = f\"image/before/\"\n",
    "    # 'html' 디렉토리가 없으면 생성\n",
    "    os.makedirs('image/before', exist_ok=True)\n",
    "    # 자동 시각화 실행\n",
    "    # max_rows_analyzed, max_cols_analyzed를 최대로 보여주기위해 150000과 수치를 입력.\n",
    "    dft = AV.AutoViz(\n",
    "        filename=\"\",\n",
    "        sep=\",\",\n",
    "        depVar=f\"{prediction}\",\n",
    "        dfte=df,\n",
    "        header=0,\n",
    "        verbose=2,\n",
    "        lowess=False,\n",
    "        chart_format=\"png\",\n",
    "        max_rows_analyzed=100,\n",
    "        max_cols_analyzed=30,\n",
    "        save_plot_dir=save_plot_dir\n",
    "    )\n",
    "    print(\"-\"*100)\n",
    "\n",
    "# 원천 데이터셋에 대한 데이터 품질 보고서 생성\n",
    "def DQ_before_save(project,dataset):\n",
    "    df = pd.read_csv(f'~/ori_data/{dataset}.csv', encoding='utf-8')\n",
    "    dq = data_cleaning_suggestions(df)\n",
    "\n",
    "    dq = dq.to_csv(f'etc/{dataset}_DQ_before.csv')\n",
    "    \n",
    "    dq = pd.read_csv(f'etc/{dataset}_DQ_before.csv',encoding='utf-8')\n",
    "    \n",
    "    dq = dq.to_json(f'json/{dataset}_DQ_before.json', orient='records')\n",
    "    \n",
    "    # json 파일 불러오기\n",
    "    with open(f'json/{dataset}_DQ_before.json', 'r') as file:\n",
    "        json_data=file.read()\n",
    "    json_api(project,'DQ_before',json_data)\n",
    "\n",
    "# 데이터셋 전처리(기본)\n",
    "def prepro_dataset(dataset, train_size,prediction):\n",
    "    df = pd.read_csv(f'~/ori_data/{dataset}.csv', encoding='utf-8')\n",
    "    ordinal_features = {col: df[col].unique() for col in df.columns if df[col].dtype == 'object'}\n",
    "    su = setup(df, target = f'{prediction}', session_id = 123,train_size=train_size, max_encoding_ohe=2, ordinal_features=ordinal_features)\n",
    "    #pycaret pull을 사용하여 이전 결과물 변수로 저장\n",
    "    config = pull(su)\n",
    "    # 아웃풋 데이터 프레임 형태로 변환\n",
    "    config = pd.DataFrame(config)\n",
    "    # json 형태로 변환\n",
    "    config = config.to_json(orient='records')\n",
    "    # json 파일 저장\n",
    "    with open(f'json/{dataset}_config.json', 'w') as file:\n",
    "        file.write(config)\n",
    "    data = get_config('dataset_transformed')\n",
    "    # 전처리 데이터 저장\n",
    "    data.to_csv(f'prepro_data/{dataset}_transformed.csv',encoding='UTF-8',index=False)\n",
    "    \n",
    "# 성인 인구조사 데이터셋 전처리\n",
    "def prepro_sample_adult(dataset,train_size,prediction):\n",
    "    df = pd.read_csv(f'~/ori_data/{dataset}.csv', encoding='utf-8')\n",
    "    su = setup(df, target = f'{prediction}', session_id = 123,train_size=train_size, max_encoding_ohe=2,\n",
    "          ordinal_features={\n",
    "              'workclass' : df['workclass'].unique(),\n",
    "              'marital-status':df['marital-status'].unique(),\n",
    "              'occupation' : df['occupation'].unique(),\n",
    "              'relationship' : df['relationship'].unique(),\n",
    "              'race' : df['race'].unique(),\n",
    "              'native-country' : df['native-country'].unique()\n",
    "          },\n",
    "          numeric_imputation= 'knn',\n",
    "          ignore_features='education',\n",
    "          use_gpu=False,   \n",
    "          feature_selection=False\n",
    "    )\n",
    "    #pycaret pull을 사용하여 이전 결과물 변수로 저장\n",
    "    config = pull(su)\n",
    "    # 아웃풋 데이터 프레임 형태로 변환\n",
    "    config = pd.DataFrame(config)\n",
    "    # json 형태로 변환\n",
    "    config = config.to_json(orient='records')\n",
    "    # json 파일 저장\n",
    "    with open(f'json/{dataset}_config.json', 'w') as file:\n",
    "        file.write(config)\n",
    "    data = get_config('dataset_transformed')\n",
    "    # 전처리 데이터 저장\n",
    "    data.to_csv(f'prepro_data/{dataset}_transformed.csv',encoding='UTF-8',index=False)\n",
    "# 대구 교통사고 예측 데이터셋 전처리\n",
    "def prepro_sample_daegu(dataset,train_size,prediction):\n",
    "    df = pd.read_csv(f'~/ori_data/{dataset}.csv', encoding='utf-8')\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    df['사고일시'] = pd.to_datetime(df['사고일시'], format='%Y-%m-%d %H')\n",
    "    su = setup(df, target = f\"{prediction}\", session_id = 123,train_size=train_size,\n",
    "               max_encoding_ohe=2,\n",
    "               # date_features =['사고일시'],\n",
    "               ordinal_features={'요일' : df['요일'].unique()},\n",
    "                categorical_features=['기상상태', '시군구', '도로형태', '노면상태', '사고유형',\n",
    "                                     '사고유형_-_세부분류', '법규위반', '가해운전자_차종',\n",
    "                                     '가해운전자_성별', '가해운전자_연령', '가해운전자_상해정도',\n",
    "                                     '피해운전자_차종', '피해운전자_성별', '피해운전자_연령',\n",
    "                                     '피해운전자_상해정도'],\n",
    "                numeric_imputation= 'knn',\n",
    "                categorical_imputation = 'mode',\n",
    "                remove_outliers=False,\n",
    "             #   date_features = ['geton_date','geton_time','getoff_date', 'getoff_time'],\n",
    "    \n",
    "             \n",
    "             ignore_features=['ID','사망자수','중상자수','경상자수','부상자수'],\n",
    "              use_gpu=False,  \n",
    "               normalize=False, \n",
    "              feature_selection=False\n",
    "              # ,\n",
    "                )\n",
    "    #pycaret pull을 사용하여 이전 결과물 변수로 저장\n",
    "    config = pull(su)\n",
    "    # 아웃풋 데이터 프레임 형태로 변환\n",
    "    config = pd.DataFrame(config)\n",
    "    # json 형태로 변환\n",
    "    config = config.to_json(orient='records')\n",
    "    # json 파일 저장\n",
    "    with open(f'json/{dataset}_config.json', 'w') as file:\n",
    "        file.write(config)\n",
    "    data = get_config('dataset_transformed')\n",
    "    # 전처리 데이터 저장\n",
    "    data.to_csv(f'prepro_data/{dataset}_transformed.csv',encoding='UTF-8',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 전처리 후 데이터 시각화 코드\n",
    "def lh_prepro_vis(dataset,prediction):\n",
    "    df_prepro = pd.read_csv(f'prepro_data/{dataset}_transformed.csv',encoding='utf-8')\n",
    "    print(\"\\n전처리 데이터 시각화 1\")\n",
    "    walke2 = pyg.walk(df_prepro)\n",
    "    with open(f\"html/{dataset}_after.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pyg.to_html(df_prepro))\n",
    "    print(\"-\"*100)\n",
    "    print(\"\\n전처리 데이터 시각화 2\")\n",
    "    # bias_before\n",
    "    %matplotlib inline\n",
    "    AV = AutoViz_Class()\n",
    "    # 시각화 결과 저장 경로\n",
    "    save_plot_dir = f\"image/after/\"\n",
    "    # 'html' 디렉토리가 없으면 생성\n",
    "    os.makedirs('image/after', exist_ok=True)\n",
    "    # 자동 시각화 실행\n",
    "    dft = AV.AutoViz(\n",
    "        filename=\"\",\n",
    "        sep=\",\",\n",
    "        depVar=f\"{prediction}\",\n",
    "        dfte=df_prepro,\n",
    "        header=0,\n",
    "        verbose=2,\n",
    "        lowess=False,\n",
    "        chart_format=\"png\",\n",
    "        max_rows_analyzed=100,\n",
    "        max_cols_analyzed=30,\n",
    "        save_plot_dir=save_plot_dir\n",
    "    )\n",
    "    print(\"-\"*100)\n",
    "# auto-viz 이미지 db 저장 코드\n",
    "def lh_image_upload(pid, base_directory, dataset,prediction):\n",
    "    if not os.path.exists(base_directory):\n",
    "        raise FileNotFoundError(f\" 주 파일경로 '{base_directory}'가 존재하지 않습니다 \")\n",
    "    \n",
    "    image_url = 'http://192.168.10.127:8000/postdb/light/insertFile/image/'\n",
    "    \n",
    "    # 테이블 이름과 해당하는 이미지 파일 이름을 매핑\n",
    "    table_image_map = {\n",
    "        \"before\": {\n",
    "            \"Scatter_Plots\": \"Scatter_Plots.png\",\n",
    "            \"Pair_Scatter_Plots\": \"Pair_Scatter_Plots.png\",\n",
    "            \"Heat_Maps\": \"Heat_Maps.png\",\n",
    "            \"Dist_Plots_target\": \"Dist_Plots_target.png\",\n",
    "            \"Dist_Plots_Numerics\": \"Dist_Plots_Numerics.png\",\n",
    "            \"Box_Plots\": \"Box_Plots.png\",\n",
    "            \"Bar_Plots\": \"Bar_Plots.png\"\n",
    "        },\n",
    "        \"after\": {\n",
    "            \"Scatter_Plots_after\": \"Scatter_Plots.png\",\n",
    "            \"Pair_Scatter_Plots_after\": \"Pair_Scatter_Plots.png\",\n",
    "            \"Heat_Maps_after\": \"Heat_Maps.png\",\n",
    "            \"Dist_Plots_target_after\": \"Dist_Plots_target.png\",\n",
    "            \"Dist_Plots_Numerics_after\": \"Dist_Plots_Numerics.png\",\n",
    "            \"Box_Plots_after\": \"Box_Plots.png\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for directory in [\"before\", \"after\"]:\n",
    "        dir_path = os.path.join(base_directory, directory, prediction)\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"경고: '{dir_path}'라는 경로가 존재하지 않습니다\")\n",
    "            continue\n",
    "        \n",
    "        for table_name, image_file in table_image_map[directory].items():\n",
    "            img_path = os.path.join(dir_path, image_file)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"경고: {img_path} 파일이 없습니다. 건너뜁니다.\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                with open(img_path, 'rb') as img_file:\n",
    "                    image_bytes = img_file.read()\n",
    "                    encoded_string = base64.b64encode(image_bytes).decode('utf-8')\n",
    "                \n",
    "                print(f\"{image_file} 진행 중...\")\n",
    "                \n",
    "                # API 요청 데이터 준비\n",
    "                image_postVal = {\n",
    "                    'val1': pid,  # 프로젝트 ID\n",
    "                    'val2': table_name,  # 테이블명\n",
    "                    'val3': encoded_string  # 텍스트로 변환된 이미지\n",
    "                }\n",
    "                \n",
    "                 # API 요청 보내기\n",
    "                response = requests.post(image_url, json=image_postVal)\n",
    "                \n",
    "                print(f\"응답 코드: {response.status_code}\")\n",
    "                print(f\"응답 내용: {response.text}\")\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    try:\n",
    "                        data = response.json()\n",
    "                        if data is None:\n",
    "                            print(f\"경고: {table_name}로부터 응답이 없습니다\")\n",
    "                        else:\n",
    "                            print(f\"{table_name}으로 전송에 성공했습니다: {data}\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"경고: {table_name}에 대해 json 파싱에 실패했습니다\")\n",
    "                        print(f\"응답 메시지: {response.text}\")\n",
    "                else:\n",
    "                    print(f'{table_name}로부터 에러가 발생했습니다: {response.status_code}')\n",
    "                    print(f'에러 메시지: {response.text}')\n",
    "                    \n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                print(f\"경고: {img_path} 파일이 없습니다.\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"{table_name}으로의 api 요청이 실패했습니다: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{image_file}을 저장하는데 예기치못한 오류가 발생했습니다.: {e}\")\n",
    "            print(\"\\n\",\"-\"*100)\n",
    "\n",
    "\n",
    "\n",
    "# 전처리 후 데이터 품질 보고서 생성\n",
    "def DQ_after_save(project,dataset):\n",
    "    df_prepro = pd.read_csv(f'prepro_data/{dataset}_transformed.csv',encoding='utf-8')\n",
    "    dq = data_cleaning_suggestions(df_prepro)\n",
    "\n",
    "    dq = dq.to_csv(f'etc/{dataset}_DQ_after.csv')\n",
    "    \n",
    "    dq = pd.read_csv(f'etc/{dataset}_DQ_after.csv',encoding='utf-8')\n",
    "    \n",
    "    dq = dq.to_json(f'json/{dataset}_DQ_after.json', orient='records')\n",
    "    \n",
    "    # json 파일 불러오기\n",
    "    with open(f'json/{dataset}_DQ_after.json', 'r') as file:\n",
    "        json_data=file.read()\n",
    "    json_api(project,'DQ',json_data)\n",
    "# \n",
    "# def lh_model_creation(project,dataset,train_size,sensitive_features,model,prediction):\n",
    "#     global selected_model\n",
    "#     df = pd.read_csv(f'~/ori_data/{dataset}.csv', encoding='utf-8')\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter(\"ignore\")\n",
    "#         with io.StringIO() as buf, redirect_stdout(buf):\n",
    "#             su = setup(df, target = f'{prediction}', session_id = 123,train_size=train_size, max_encoding_ohe=2,\n",
    "#                        ordinal_features={\n",
    "#                           'workclass' : df['workclass'].unique(),\n",
    "#                           'marital-status':df['marital-status'].unique(),\n",
    "#                           'occupation' : df['occupation'].unique(),\n",
    "#                           'relationship' : df['relationship'].unique(),\n",
    "#                           'race' : df['race'].unique(),\n",
    "#                           'native-country' : df['native-country'].unique()\n",
    "#                        },\n",
    "#                        numeric_imputation= 'knn',\n",
    "#                        ignore_features='education',\n",
    "#                        use_gpu=False,   \n",
    "#                        feature_selection=False,\n",
    "#                        verbose=False\n",
    "#                        )\n",
    "#     print(\"\\n학습 모델 상호평가 리스트\")\n",
    "#     best=compare_models(exclude='lightgbm')\n",
    "#     training=pull(best)\n",
    "\n",
    "#     training = pd.DataFrame(training)\n",
    "    \n",
    "#     training = training.to_json(orient='records')\n",
    "    \n",
    "#     # 학습한 모델 목록 저장\n",
    "#     with open(f'json/{dataset}_model_list.json', 'w') as file:\n",
    "#         file.write(training)\n",
    "    \n",
    "#     with open(f'json/{dataset}_model_list.json', 'r') as file:\n",
    "#         json_data=file.read()\n",
    "    \n",
    "#     # DB 저장 api 호출\n",
    "#     json_api(project,'model',json_data)\n",
    "#     print(\"\\n\",\"-\"*100)\n",
    "\n",
    "#     # 생성하고자 하는 모델 입력\n",
    "#     # selected_model = input('위 리스트로부터 생성하려는 모델을 입력하십시오:')\n",
    "#     selected_model = f'{model}'\n",
    "    \n",
    "#     # 머신러닝 모델 선정\n",
    "#     print(\"\\n선택한 모델을 생성\")\n",
    "#     model = create_model(f'{selected_model}')\n",
    "#     print(\"\\n\",\"-\"*100)\n",
    "#     # tune_model을 통해 모델의 성능을 조금 더 강화할 수 있음\n",
    "#     print(\"\\n모델 튜닝\")\n",
    "#     enhanced_model= tune_model(model)\n",
    "#     clf1 = get_current_experiment()\n",
    "#     # print(plot_model(enhanced_model, plot = 'auc'),\n",
    "#     #       plot_model(enhanced_model, plot = 'threshold'),\n",
    "#     #       plot_model(enhanced_model, plot = 'pr'), \n",
    "#     #       plot_model(enhanced_model, plot = 'confusion_matrix'),\n",
    "#     #       plot_model(enhanced_model, plot = 'feature'),\n",
    "#     #      # interpret_model(tuned_rf)\n",
    "#     #      )\n",
    "#     # # 표본 개수 지정\n",
    "#     # N = 500\n",
    "#     # X = df_prepro.drop('income',axis=1)[:N]\n",
    "#     # y = df_prepro['income'][:N]\n",
    "#     # 원하는 디렉토리 경로 설정\n",
    "#     print(\"\\n\",\"-\"*100)\n",
    "#     print(\"\\n모델 성능 시각화\")\n",
    "#     desired_directory = \"image/performance/\"\n",
    "    \n",
    "#     # 디렉토리가 없으면 생성\n",
    "#     os.makedirs(desired_directory, exist_ok=True)\n",
    "    \n",
    "#     # plot_model() 함수 사용 시 디렉토리 경로만 지정\n",
    "#     plot_model(enhanced_model, plot='auc', save=desired_directory)\n",
    "#     plot_model(enhanced_model, plot='confusion_matrix', save=desired_directory)\n",
    "#     plot_model(enhanced_model, plot='threshold', save=desired_directory)\n",
    "#     plot_model(enhanced_model, plot='pr', save=desired_directory)\n",
    "#     plot_model(enhanced_model, plot = 'feature', save=desired_directory)\n",
    "#     # interpret_model(enhanced_model, X_new_sample=get_config('X_transformed')[:N], y_new_sample=get_config('y_transformed')[:N],save=desired_directory)\n",
    "#     # interpret_model(enhanced_model, save=desired_directory)\n",
    "#     # 파일 이름 변경 (필요한 경우)\n",
    "#     os.rename(os.path.join(desired_directory, \"AUC.png\"), os.path.join(desired_directory, f\"{dataset}_AUC.png\"))\n",
    "#     os.rename(os.path.join(desired_directory, \"Confusion Matrix.png\"), os.path.join(desired_directory, f\"{dataset}_Confusion Matrix.png\"))\n",
    "#     os.rename(os.path.join(desired_directory, \"Threshold.png\"), os.path.join(desired_directory, f\"{dataset}_Threshold.png\"))\n",
    "#     os.rename(os.path.join(desired_directory, \"Precision Recall.png\"), os.path.join(desired_directory, f\"{dataset}_Precision Recall.png\"))\n",
    "#     os.rename(os.path.join(desired_directory, \"Feature Importance.png\"), os.path.join(desired_directory, f\"{dataset}_Feature Importance.png\"))\n",
    "#     # os.rename(os.path.join(desired_directory, \"SHAP summary.png\"), os.path.join(desired_directory, f\"{dataset}_SHAP summary.png\"))\n",
    "    \n",
    "#     pycaret_image_upload(project,'image/performance',dataset)\n",
    "#     print(\"\\n\",\"-\"*100)\n",
    "#     print(\"\\n모델 저장\")\n",
    "#     with open(f'pkl/{dataset}_{selected_model}.pkl', 'wb') as file:\n",
    "#         pickle.dump(enhanced_model, file)\n",
    "\n",
    "#     print(f\"모델이 '{dataset}_{selected_model}.pkl' 파일로 저장되었습니다.\")\n",
    "\n",
    "\n",
    "# 모델 저장 코드\n",
    "def model_saving(dataset,model):\n",
    "    print(\"\\n모델 저장\")\n",
    "    with open(f'pkl/{dataset}_{model}.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print(f\"모델이 '{dataset}_{model}.pkl' 파일로 저장되었습니다.\")\n",
    "# 모델 성능지표 시각화 코드\n",
    "def model_vis(dataset, project_type, model):\n",
    "    if project_type == '분류':\n",
    "        with open(f'pkl/{dataset}_{model}.pkl', 'rb') as file:\n",
    "            loaded_model = pickle.load(file)\n",
    "        print(\"\\n모델 성능 시각화\")\n",
    "        desired_directory = \"image/performance/\"\n",
    "        # 디렉토리가 없으면 생성\n",
    "        os.makedirs(desired_directory, exist_ok=True)\n",
    "        \n",
    "        # plot_model() 함수 사용 시 디렉토리 경로만 지정\n",
    "        plot_model(loaded_model, plot='auc', save=desired_directory)\n",
    "        plot_model(loaded_model, plot='confusion_matrix', save=desired_directory)\n",
    "        plot_model(loaded_model, plot='threshold', save=desired_directory)\n",
    "        plot_model(loaded_model, plot='pr', save=desired_directory)\n",
    "        plot_model(loaded_model, plot = 'feature', save=desired_directory)\n",
    "        # interpret_model(enhanced_model, X_new_sample=get_config('X_transformed')[:N], y_new_sample=get_config('y_transformed')[:N],save=desired_directory)\n",
    "        # interpret_model(enhanced_model, save=desired_directory)\n",
    "        # 파일 이름 변경 (필요한 경우)\n",
    "        os.rename(os.path.join(desired_directory, \"AUC.png\"), os.path.join(desired_directory, f\"{dataset}_AUC.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Confusion Matrix.png\"), os.path.join(desired_directory, f\"{dataset}_Confusion Matrix.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Threshold.png\"), os.path.join(desired_directory, f\"{dataset}_Threshold.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Precision Recall.png\"), os.path.join(desired_directory, f\"{dataset}_Precision Recall.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Feature Importance.png\"), os.path.join(desired_directory, f\"{dataset}_Feature Importance.png\"))\n",
    "        # os.rename(os.path.join(desired_directory, \"SHAP summary.png\"), os.path.join(desired_directory, f\"{dataset}_SHAP summary.png\"))\n",
    "        print(\"\\n\",\"-\"*100)\n",
    "    elif project_type == '회귀':\n",
    "        with open(f'pkl/{dataset}_{model}.pkl', 'rb') as file:\n",
    "            loaded_model = pickle.load(file)\n",
    "        print(\"\\n모델 성능 시각화\")\n",
    "        desired_directory = \"image/performance/\"\n",
    "        # 디렉토리가 없으면 생성\n",
    "        os.makedirs(desired_directory, exist_ok=True)\n",
    "        \n",
    "        # plot_model() 함수 사용 시 디렉토리 경로만 지정\n",
    "        plot_model(loaded_model, plot='residuals', save=desired_directory)\n",
    "        plot_model(loaded_model, plot='error', save=desired_directory)\n",
    "        plot_model(loaded_model, plot='cooks', save=desired_directory)\n",
    "        plot_model(loaded_model, plot='learning', save=desired_directory)\n",
    "        plot_model(loaded_model, plot = 'feature', save=desired_directory)\n",
    "        # interpret_model(enhanced_model, X_new_sample=get_config('X_transformed')[:N], y_new_sample=get_config('y_transformed')[:N],save=desired_directory)\n",
    "        # interpret_model(enhanced_model, save=desired_directory)\n",
    "        # 파일 이름 변경 (필요한 경우)\n",
    "        os.rename(os.path.join(desired_directory, \"Residuals.png\"), os.path.join(desired_directory, f\"{dataset}_Residuals.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Prediction Error.png\"), os.path.join(desired_directory, f\"{dataset}_Prediction Error.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Cooks Distance.png\"), os.path.join(desired_directory, f\"{dataset}_Cooks Distance.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Learning Curve.png\"), os.path.join(desired_directory, f\"{dataset}_Learning Curve.png\"))\n",
    "        os.rename(os.path.join(desired_directory, \"Feature Importance.png\"), os.path.join(desired_directory, f\"{dataset}_Feature Importance.png\"))\n",
    "        # os.rename(os.path.join(desired_directory, \"SHAP summary.png\"), os.path.join(desired_directory, f\"{dataset}_SHAP summary.png\"))\n",
    "        print(\"\\n\",\"-\"*100)\n",
    "        \n",
    "\n",
    "# 추천모델의 편향성 포함 성능지표(pycaret)\n",
    "def lh_fairness(project,project_type,dataset,model,sensitive_features):\n",
    "    #모델 편향성 체크\n",
    "    print(\"\\n모델 편향성 체크\")\n",
    "    # 저장한 모델 불러오기\n",
    "    with open(f'pkl/{dataset}_{model}.pkl', 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    rf_fairness = check_fairness(loaded_model, sensitive_features = sensitive_features)\n",
    "    fair=pull()\n",
    "    #json변환\n",
    "    convert_pycaretfair_to_json(project_type, fair, dataset)\n",
    "    with open(f'json/{dataset}_fairness.json', 'r') as file:\n",
    "        json_data = file.read()\n",
    "    # DB 저장 api 호출\n",
    "    json_api(project,'et_fairness',json_data)\n",
    "\n",
    "# 추천모델의 편향성 포함 성능지표2(pycaret)\n",
    "def lh_custom_fairness(project,dataset, model, X, y, sensitive_features):\n",
    "    # 모델 로드\n",
    "    with open(f'pkl/{dataset}_{model}.pkl', 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    \n",
    "    # 데이터 준비\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # PyCaret 실험 객체 생성 및 설정\n",
    "    exp = ClassificationExperiment()\n",
    "    exp = setup(data=data, target=y.name, session_id=42, verbose=False, \n",
    "                preprocess=False, \n",
    "                data_split_shuffle=False,\n",
    "                data_split_stratify=False)\n",
    "    \n",
    "    # Fairness 검사 수행\n",
    "    try:\n",
    "        fairness_results = exp.check_fairness(\n",
    "            estimator=loaded_model, \n",
    "            sensitive_features=sensitive_features\n",
    "        )\n",
    "    except AttributeError as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        print(\"PyCaret의 최신 버전에서 변경된 API를 사용해 보겠습니다.\")\n",
    "        fairness_results = check_fairness(\n",
    "            estimator=loaded_model,\n",
    "            sensitive_features=sensitive_features\n",
    "        )\n",
    "    fair=pull()\n",
    "    convert_pycaretfair_to_json(project_type,fair, dataset)\n",
    "    with open(f'json/{dataset}_fairness.json', 'r') as file:\n",
    "        json_data = file.read()\n",
    "    # DB 저장 api 호출\n",
    "    json_api(project,'et_fairness',json_data)\n",
    "\n",
    "# 모델 편향성 데이터프레임 생성(fairlearn)\n",
    "def model_bias_check(project,project_type,dataset, sensitive_features,train_size,prediction,model):\n",
    "    \n",
    "        # 전처리된 데이터 불러오기\n",
    "        df_prepro = pd.read_csv(f'prepro_data/{dataset}_transformed.csv', encoding='utf-8')\n",
    "        # 예측값 제거\n",
    "        X = df_prepro.drop([f'{prediction}'], axis=1)\n",
    "        y = df_prepro[f'{prediction}']\n",
    "        sensitive_group_all = df_prepro[sensitive_features]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "            X, y, sensitive_group_all, train_size=train_size, random_state=999)\n",
    "        if project_type == '분류':\n",
    "            # 저장한 모델 불러오기\n",
    "            with open(f'pkl/{dataset}_{model}.pkl', 'rb') as file:\n",
    "                loaded_model = pickle.load(file)\n",
    "            # 임시 클로드 파이썬 파일\n",
    "            constraint = DemographicParity()\n",
    "            mitigator = ExponentiatedGradient(loaded_model, constraint)\n",
    "            mitigator.fit(X_train, y_train, sensitive_features=X_train[sensitive_features])\n",
    "            # 예측\n",
    "            y_pred_original = loaded_model.predict(X_test)\n",
    "            y_pred_mitigated = mitigator.predict(X_test)\n",
    "            def calculate_metrics(y_true, y_pred, sensitive_features, col_nm='all'):\n",
    "                return pd.DataFrame({\n",
    "                    'date': date.today().strftime('%Y-%m-%d'),\n",
    "                    'col_nm': col_nm,\n",
    "                    'demographic_parity_difference': [demographic_parity_difference(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "                    'demographic_parity_ratio': [demographic_parity_ratio(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "                    'equalized_odds_difference': [equalized_odds_difference(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "                    'equalized_odds_ratio': [equalized_odds_ratio(y_true, y_pred, sensitive_features=sensitive_features)]\n",
    "                })\n",
    "            \n",
    "            # 원본 모델과 완화된 모델의 결과 비교\n",
    "            fairlearn_data_original = []\n",
    "            fairlearn_data_mitigated = []\n",
    "            \n",
    "            # 모든 특성을 고려한 공정성 지표 계산 ('all'로 설정)\n",
    "            fairlearn_data_original.append(calculate_metrics(y_test, y_pred_original, A_test, col_nm='all'))\n",
    "            fairlearn_data_mitigated.append(calculate_metrics(y_test, y_pred_mitigated, A_test, col_nm='all'))\n",
    "            \n",
    "            # 개별 민감한 특성에 대한 지표 계산\n",
    "            for feature in sensitive_features:\n",
    "                fairlearn_data_original.append(calculate_metrics(y_test, y_pred_original, A_test[[feature]], col_nm=feature))\n",
    "                fairlearn_data_mitigated.append(calculate_metrics(y_test, y_pred_mitigated, A_test[[feature]], col_nm=feature))\n",
    "            \n",
    "            # 모든 결과를 각각 DataFrame으로 결합\n",
    "            fairlearn_original = pd.concat(fairlearn_data_original, ignore_index=True)\n",
    "            fairlearn_mitigated = pd.concat(fairlearn_data_mitigated, ignore_index=True)\n",
    "        \n",
    "            # JSON으로 변환 및 저장\n",
    "            fairlearn_original_json = fairlearn_original.to_json(orient='records')\n",
    "            fairlearn_mitigated_json = fairlearn_mitigated.to_json(orient='records')\n",
    "        \n",
    "            # API로 결과 전송\n",
    "            json_api(project, 'fairlearn_before', fairlearn_original_json)\n",
    "            json_api(project, 'fairlearn_after', fairlearn_mitigated_json)\n",
    "        \n",
    "            print(\"\\n편향 전후 비교\")\n",
    "            print(\"\\n편향 제거 이전:\")\n",
    "            print(fairlearn_original)\n",
    "            print(\"\\n\", \"-\"*100)\n",
    "            print(\"\\n편향 제거 이후:\")\n",
    "            print(fairlearn_mitigated)\n",
    "            \n",
    "            # 설명 문자열 생성\n",
    "            explanation = (\"\\ndemographic_parity_difference(인구 동등성 차이): 머신러닝 예측 확률이 민감 집단으로 인해 영향받지 않는 정도의 차이. 0에 가까울수록 적당\\n\" \n",
    "                           \"demographic_parity_ratio(인구 동등성 비율): 머신러닝 예측 확률이 민감 집단으로 인해 영향받지 않는 정도의 차이. 1에 가까울수록 적당\\n\"\n",
    "                           \"equalized_odds_difference(균등화된 확률 차이): 예측 정도가 민감 집단에 영향받지 않는 정도는 물론, FP와 TP가 같은 수치를 보이고 있음을 나타내는 정도이다. 추가로 FP란 실제값이 거짓인데 참이라고 예측한 확률이며, TP는 실제값이 참이고, 참이라고 예측한 확률을 의미한다. 0에 가까울수록 모델이 더 공정. 0에서 멀어질수록 편향적.\\n\"\n",
    "                           \"equalized_odds_ratio(균등 기회 확률): 머신러닝 모델의 예측 정확도가 서로 다른 민감 집단 간에 얼마나 일관되는지를 나타내는 비율. 1에 가까울수록 모델이 더 공정하다고 판단. 1에서 크게 벗어난 값은 특정 집단에 대한 편향이 존재한다고 추정 가능\")\n",
    "            print(explanation)\n",
    "            return fairlearn_mitigated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif project_type == '회귀':\n",
    "            \n",
    "                   # 저장한 모델 불러오기\n",
    "            with open(f'pkl/{dataset}_{model}.pkl', 'rb') as file:\n",
    "                loaded_model = pickle.load(file)\n",
    "            \n",
    "            # 손실 함수 클래스 정의\n",
    "            class SquaredLoss:\n",
    "                def __init__(self):\n",
    "                    pass\n",
    "        \n",
    "                def eval(self, y_true, y_pred):\n",
    "                    return (y_true - y_pred) ** 2\n",
    "        \n",
    "            # 상한값 계산\n",
    "            y_pred = loaded_model.predict(X_train)\n",
    "            squared_errors = (y_train - y_pred) ** 2\n",
    "            upper_bound = np.mean(squared_errors) + 2 * np.std(squared_errors)\n",
    "        \n",
    "            # BoundedGroupLoss와 ExponentiatedGradient를 사용한 편향 완화\n",
    "            constraint = BoundedGroupLoss(SquaredLoss(), upper_bound=upper_bound)\n",
    "            mitigator = ExponentiatedGradient(estimator=loaded_model, constraints=constraint)\n",
    "            mitigator.fit(X_train, y_train, sensitive_features=A_train)\n",
    "            \n",
    "            # 예측\n",
    "            y_pred_original = loaded_model.predict(X_test)\n",
    "            y_pred_mitigated = mitigator.predict(X_test)\n",
    "            \n",
    "            def calculate_metrics(y_true, y_pred, sensitive_features, col_nm='all'):\n",
    "                group_min_metric = make_derived_metric(metric=mean_prediction, transform='group_min')\n",
    "                group_max_metric = make_derived_metric(metric=mean_prediction, transform='group_max')\n",
    "                \n",
    "                group_mse = {}\n",
    "                for feature in sensitive_features.columns:\n",
    "                    group_mse[feature] = {}\n",
    "                    for group in sensitive_features[feature].unique():\n",
    "                        mask = sensitive_features[feature] == group\n",
    "                        group_mse[feature][group] = mean_squared_error(y_true[mask], y_pred[mask])\n",
    "                \n",
    "                return pd.DataFrame({\n",
    "                    'date': [date.today().strftime('%Y-%m-%d')],\n",
    "                    'col_nm': [col_nm],\n",
    "                    'mean_prediction': [mean_prediction(y_true, y_pred)],\n",
    "                    'group_min_mean_prediction': [group_min_metric(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "                    'group_max_mean_prediction': [group_max_metric(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "                    'mean_prediction_gap': [group_max_metric(y_true, y_pred, sensitive_features=sensitive_features) - \n",
    "                                            group_min_metric(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "                    'mse': [mean_squared_error(y_true, y_pred)],\n",
    "                    'max_group_mse': [max(max(mse.values()) for mse in group_mse.values())]\n",
    "                })\n",
    "            \n",
    "            # 원본 모델과 완화된 모델의 결과 비교\n",
    "            fairlearn_data_original = []\n",
    "            fairlearn_data_mitigated = []\n",
    "            \n",
    "            # 모든 특성을 고려한 공정성 지표 계산 ('all'로 설정)\n",
    "            fairlearn_data_original.append(calculate_metrics(y_test, y_pred_original, A_test, col_nm='all'))\n",
    "            fairlearn_data_mitigated.append(calculate_metrics(y_test, y_pred_mitigated, A_test, col_nm='all'))\n",
    "            \n",
    "            # 개별 민감한 특성에 대한 지표 계산\n",
    "            for feature in sensitive_features:\n",
    "                fairlearn_data_original.append(calculate_metrics(y_test, y_pred_original, A_test[[feature]], col_nm=feature))\n",
    "                fairlearn_data_mitigated.append(calculate_metrics(y_test, y_pred_mitigated, A_test[[feature]], col_nm=feature))\n",
    "            \n",
    "            # 모든 결과를 각각 DataFrame으로 결합\n",
    "            fairlearn_original = pd.concat(fairlearn_data_original, ignore_index=True)\n",
    "            fairlearn_mitigated = pd.concat(fairlearn_data_mitigated, ignore_index=True)\n",
    "        \n",
    "            # JSON으로 변환 및 저장\n",
    "            fairlearn_original_json = fairlearn_original.to_json(orient='records')\n",
    "            fairlearn_mitigated_json = fairlearn_mitigated.to_json(orient='records')\n",
    "        \n",
    "            # API로 결과 전송\n",
    "            json_api(project, 'fairlearn_before', fairlearn_original_json)\n",
    "            json_api(project, 'fairlearn_after', fairlearn_mitigated_json)\n",
    "        \n",
    "            print(\"\\n편향 전후 비교\")\n",
    "            print(\"\\n편향 제거 이전:\")\n",
    "            print(fairlearn_original)\n",
    "            print(\"\\n\", \"-\"*100)\n",
    "            print(\"\\n편향 제거 이후:\")\n",
    "            print(fairlearn_mitigated)\n",
    "            \n",
    "            # 설명 문자열 생성\n",
    "            explanation = (\"\\nmean_prediction: 전체 데이터셋에 대한 평균 예측값\\n\"\n",
    "                           \"group_min_mean_prediction: 그룹별 평균 예측값 중 최소값\\n\"\n",
    "                           \"group_max_mean_prediction: 그룹별 평균 예측값 중 최대값\\n\"\n",
    "                           \"mean_prediction_gap: 그룹별 평균 예측값의 최대-최소 차이. 작을수록 더 공정\\n\"\n",
    "                           \"mse: 전체 데이터셋에 대한 평균 제곱 오차\\n\"\n",
    "                           \"max_group_mse: 모든 그룹 중 가장 큰 평균 제곱 오차. 작을수록 더 공정\")\n",
    "            print(explanation)\n",
    "            \n",
    "            return fairlearn_mitigated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 시스템 편향 코드\n",
    "def system_bias(pid, dataset, prediction, sensitive_features, train_size, model):\n",
    "    df_prepro = pd.read_csv(f'prepro_data/{dataset}_transformed.csv', encoding='utf-8')\n",
    "    N = 50\n",
    "    X = df_prepro.drop(f'{prediction}', axis=1)[:N]\n",
    "    y = df_prepro[f'{prediction}'][:N]\n",
    "    # sensitive_features가 문자열이라면 리스트로 변환\n",
    "    if isinstance(sensitive_features, str):\n",
    "        sensitive_features = [sensitive_features]\n",
    "\n",
    "    sensitive_group_all = df_prepro[sensitive_features][:N]  # 수정된 부분\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "        X, y, sensitive_group_all, train_size=train_size, random_state=999)\n",
    "    \n",
    "    # 저장한 모델 불러오기\n",
    "    with open(f'pkl/{dataset}_{model}.pkl', 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    \n",
    "    # SHAP 값 계산\n",
    "    explainer = shap.Explainer(loaded_model, X_train)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    # 인덱스 접근 조정\n",
    "    if len(shap_values.shape) == 2:  # (샘플 수, 피쳐 수)\n",
    "        class_index = 0  # 첫 번째 클래스를 선택\n",
    "        shap_value_to_plot = shap_values[0, :]  # 첫 번째 샘플의 SHAP 값\n",
    "    elif len(shap_values.shape) == 3:  # (샘플 수, 피쳐 수, 클래스 수)\n",
    "        class_index = 1  # 예: 두 번째 클래스를 선택\n",
    "        shap_value_to_plot = shap_values[0, :, class_index]  # 첫 번째 샘플의 특정 클래스 SHAP 값\n",
    "    else:\n",
    "        raise ValueError(\"SHAP 값의 차원이 예상과 다릅니다.\")\n",
    "        \n",
    "    # Waterfall 차트 생성\n",
    "    image_path = os.path.abspath('image/performance')\n",
    "    os.makedirs(image_path, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "    file_path = os.path.join(image_path, f'{dataset}_waterfall.png')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap_value_to_plot, show=False)  # show=False로 설정\n",
    "    plt.tight_layout()\n",
    "    plt.draw()  # 그래프 렌더링\n",
    "    print(f\"Saving figure to: {file_path}\")\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # 메모리 해제\n",
    "    \n",
    "    print(f\"Figure saved. File exists: {os.path.exists(file_path)}\")\n",
    "    \n",
    "    # 저장된 이미지를 base64로 인코딩\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    # DB에 저장\n",
    "    image_url = 'http://192.168.10.127:8000/postdb/light/insertFile/image/'\n",
    "    \n",
    "    image_postVal = {\n",
    "        'val1': pid,\n",
    "        'val2': 'waterfall',\n",
    "        'val3': encoded_image\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(image_url, json=image_postVal)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"Waterfall 차트 저장 완료했습니다: {data}\")\n",
    "        else:\n",
    "            print(f'Waterfall 차트 저장 중 오류가 발생했습니다: {response.status_code}')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Waterfall 차트 API 요청이 실패했습니다: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Waterfall 차트를 저장하는데 예기치못한 오류가 발생했습니다.: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def confirm_log(pid,date, model, count, status):\n",
    "    confirm_url = 'http://192.168.10.127:8000/postdb/light/insertMap/confirm/'\n",
    "    confirm_postVal = {\n",
    "            'val1': pid, # 프로젝트 ID\n",
    "            'val2': 'string' , # confirm code?\n",
    "            'val3': date,\n",
    "            'val4': model,\n",
    "            'val5': count,\n",
    "            'val6': status,\n",
    "            'val7': 'string',\n",
    "        # confirm 데이터 or 텍스트로 변환된 이미지\n",
    "    }\n",
    "    response = requests.post(confirm_url,confirm = confirm_postVal)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # JSON 형식의 응답 데이터를 파이썬 딕셔너리로 변환\n",
    "        print(f\"Confirm id {data} 로 저장 완료됨\")\n",
    "    else:\n",
    "        print(f'오류: {response.status_code}')\n",
    "    \n",
    "\n",
    "\n",
    "def train_log(pid,date, data, count, status):\n",
    "    train_url = 'http://192.168.10.127:8000/postdb/light/insertMap/train/'\n",
    "    train_postVal = {\n",
    "            'val1': pid, # 프로젝트 ID\n",
    "            'val2': 'string' , # train code?\n",
    "            'val3': date,\n",
    "            'val4': data,\n",
    "            'val5': count,\n",
    "            'val6': status,\n",
    "            'val7': 'string',\n",
    "        # train 데이터 or 텍스트로 변환된 이미지\n",
    "    }\n",
    "    response = requests.post(train_url,train = train_postVal)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # JSON 형식의 응답 데이터를 파이썬 딕셔너리로 변환\n",
    "        print(f\"Train id {data} 로 저장 완료됨\")\n",
    "    else:\n",
    "        print(f'오류: {response.status_code}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# def model_bias_check(project,dataset, sensitive_features,train_size,prediction):\n",
    "#     global selected_model\n",
    "#     # 전처리된 데이터 불러오기\n",
    "#     df_prepro = pd.read_csv(f'prepro_data/{dataset}_transformed.csv', encoding='utf-8')\n",
    "#     # 예측값 제거\n",
    "#     # 예측값 제거\n",
    "#     X = df_prepro.drop([f'{prediction}'], axis=1)\n",
    "#     y = df_prepro[f'{prediction}']\n",
    "#     sensitive_group_all = df_prepro[sensitive_features]\n",
    "#     X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "#         X, y, sensitive_group_all, train_size=train_size, random_state=999)\n",
    "#     # 저장한 모델 불러오기\n",
    "#     with open(f'pkl/{dataset}_{selected_model}.pkl', 'rb') as file:\n",
    "#         loaded_model = pickle.load(file)\n",
    "#     # 임시 클로드 파이썬 파일\n",
    "#     constraint = DemographicParity()\n",
    "#     mitigator = ExponentiatedGradient(loaded_model, constraint)\n",
    "#     mitigator.fit(X_train, y_train, sensitive_features=X_train[sensitive_features])\n",
    "#     # 예측\n",
    "#     y_pred_original = loaded_model.predict(X_test)\n",
    "#     y_pred_mitigated = mitigator.predict(X_test)\n",
    "    \n",
    "#     def calculate_metrics(y_true, y_pred, sensitive_features, col_nm='all'):\n",
    "#         return pd.DataFrame({\n",
    "#             'date': date.today().strftime('%Y-%m-%d'),\n",
    "#             'col_nm': col_nm,\n",
    "#             'demographic_parity_difference': [demographic_parity_difference(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "#             'demographic_parity_ratio': [demographic_parity_ratio(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "#             'equalized_odds_difference': [equalized_odds_difference(y_true, y_pred, sensitive_features=sensitive_features)],\n",
    "#             'equalized_odds_ratio': [equalized_odds_ratio(y_true, y_pred, sensitive_features=sensitive_features)]\n",
    "#         })\n",
    "    \n",
    "#     # 원본 모델과 완화된 모델의 결과 비교\n",
    "#     fairlearn_data_original = []\n",
    "#     fairlearn_data_mitigated = []\n",
    "    \n",
    "#     # 모든 특성을 고려한 공정성 지표 계산 ('all'로 설정)\n",
    "#     fairlearn_data_original.append(calculate_metrics(y_test, y_pred_original, X_test[sensitive_features], col_nm='all'))\n",
    "#     fairlearn_data_mitigated.append(calculate_metrics(y_test, y_pred_mitigated, X_test[sensitive_features], col_nm='all'))\n",
    "    \n",
    "#     # 개별 민감한 특성에 대한 지표 계산\n",
    "#     for feature in sensitive_features:\n",
    "#         fairlearn_data_original.append(calculate_metrics(y_test, y_pred_original, X_test[[feature]], col_nm=feature))\n",
    "#         fairlearn_data_mitigated.append(calculate_metrics(y_test, y_pred_mitigated, X_test[[feature]], col_nm=feature))\n",
    "    \n",
    "#     # 모든 결과를 각각 DataFrame으로 결합\n",
    "#     fairlearn_original = pd.concat(fairlearn_data_original, ignore_index=True)\n",
    "#     # 변환\n",
    "#     convert_fairlearn_to_json(fairlearn_original, dataset)\n",
    "    \n",
    "#     with open(f'json/{dataset}_fairlearn.json', 'r') as file:\n",
    "#         json_data = file.read()\n",
    "#     #저장\n",
    "#     json_api(project,'fairlearn_before',json_data)\n",
    "    \n",
    "#     fairlearn_mitigated = pd.concat(fairlearn_data_mitigated, ignore_index=True)\n",
    "#     # 변환\n",
    "#     convert_fairlearn_to_json(fairlearn_mitigated, dataset)\n",
    "    \n",
    "#     with open(f'json/{dataset}_fairlearn.json', 'r') as file:\n",
    "#         json_data = file.read()\n",
    "#     #저장\n",
    "#     json_api(project,'fairlearn_after',json_data)\n",
    "#     print(\"\\n편향 전후 비교\")\n",
    "#     # 결과 출력\n",
    "#     print(\"\\n편향 제거 이전:\")\n",
    "#     print(fairlearn_original)\n",
    "#     print(\"\\n\",\"-\"*100)\n",
    "#     print(\"\\n편향 제거 이후:\")\n",
    "#     print(fairlearn_mitigated)\n",
    "    \n",
    "#     # 설명 문자열 생성\n",
    "#     explanation = (\"\\ndemographic_parity_difference(인구 동등성 차이): 머신러닝 예측 확률이 민감 집단으로 인해 영향받지 않는 정도의 차이. 0에 가까울수록 적당\\n\" \n",
    "#                    \"demographic_parity_ratio(인구 동등성 비율): 머신러닝 예측 확률이 민감 집단으로 인해 영향받지 않는 정도의 차이. 1에 가까울수록 적당\\n\"\n",
    "#                    \"equalized_odds_difference(균등화된 확률 차이): 예측 정도가 민감 집단에 영향받지 않는 정도는 물론, FP와 TP가 같은 수치를 보이고 있음을 나타내는 정도이다. 추가로 FP란 실제값이 거짓인데 참이라고 예측한 확률이며, TP는 실제값이 참이고, 참이라고 예측한 확률을 의미한다. 0에 가까울수록 모델이 더 공정. 0에서 멀어질수록 편향적.\\n\"\n",
    "#                    \"equalized_odds_ratio(균등 기회 확률): 머신러닝 모델의 예측 정확도가 서로 다른 민감 집단 간에 얼마나 일관되는지를 나타내는 비율. 1에 가까울수록 모델이 더 공정하다고 판단. 1에서 크게 벗어난 값은 특정 집단에 대한 편향이 존재한다고 추정 가능\")\n",
    "#     print(explanation)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a1f2d9-9aa7-4eff-a6cc-67de9041209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_bias_check(project,dataset, sensitive_features,train_size,prediction):\n",
    "#     # 전처리된 데이터 불러오기\n",
    "#     df_prepro = pd.read_csv(f'prepro_data/{dataset}_transformed.csv', encoding='utf-8')\n",
    "    \n",
    "#     # 예측값 제거\n",
    "#     X = df_prepro.drop([f'{prediction}'] + sensitive_features, axis=1)\n",
    "#     y = df_prepro[f'{prediction}']\n",
    "\n",
    "#     # 민감한 특성별 한글 이름 지정\n",
    "#     # feature_names_mapping = {\n",
    "#     #     'gender': '성별',\n",
    "#     #     'race': '인종'\n",
    "#     # }\n",
    "\n",
    "#     # 민감한 특성별 공정성 지표를 계산해 JSON으로 변환\n",
    "#     fairlearn_data = []\n",
    "\n",
    "#     # 모든 민감한 특성을 동시에 고려한 지표 계산\n",
    "#     sensitive_group_all = df_prepro[sensitive_features]\n",
    "#     X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "#         X, y, sensitive_group_all, train_size=train_size, random_state=999\n",
    "#     )\n",
    "    \n",
    "#     # 저장한 모델 불러오기\n",
    "#     with open(f'pkl/{dataset}_random_forest_model.pkl', 'rb') as file:\n",
    "#         loaded_model = pickle.load(file)\n",
    "    \n",
    "#     loaded_model.fit(X_train, y_train)\n",
    "#     y_pred = loaded_model.predict(X_test)\n",
    "    \n",
    "#     # 모든 특성을 고려한 공정성 지표 계산\n",
    "#     fairlearn_metrics_all = pd.DataFrame({\n",
    "#         'date': date.today().strftime('%Y-%m-%d'),\n",
    "#         'col_nm': 'all',\n",
    "#         'demographic_parity_difference': [demographic_parity_difference(y_test, y_pred, sensitive_features=A_test)],\n",
    "#         'demographic_parity_ratio': [demographic_parity_ratio(y_test, y_pred, sensitive_features=A_test)],\n",
    "#         'equalized_odds_difference': [equalized_odds_difference(y_test, y_pred, sensitive_features=A_test)],\n",
    "#         'equalized_odds_ratio': [equalized_odds_ratio(y_test, y_pred, sensitive_features=A_test)]\n",
    "#     })\n",
    "#     fairlearn_data.append(fairlearn_metrics_all)\n",
    "    \n",
    "#     # 개별 민감한 특성에 대한 지표 계산\n",
    "#     for feature in sensitive_features:\n",
    "#         # sensitive_features_nm = feature_names_mapping.get(feature, feature)\n",
    "#         sensitive_features_nm =  feature\n",
    "#         sensitive_group = df_prepro[[feature]]\n",
    "        \n",
    "#         X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "#             X, y, sensitive_group, train_size=train_size, random_state=999\n",
    "#         )\n",
    "        \n",
    "#         y_pred = loaded_model.predict(X_test)\n",
    "        \n",
    "#         fairlearn_metrics = pd.DataFrame({\n",
    "#             'date': date.today().strftime('%Y-%m-%d'),\n",
    "#             'col_nm': sensitive_features_nm,\n",
    "#             'demographic_parity_difference': [demographic_parity_difference(y_test, y_pred, sensitive_features=A_test)],\n",
    "#             'demographic_parity_ratio': [demographic_parity_ratio(y_test, y_pred, sensitive_features=A_test)],\n",
    "#             'equalized_odds_difference': [equalized_odds_difference(y_test, y_pred, sensitive_features=A_test)],\n",
    "#             'equalized_odds_ratio': [equalized_odds_ratio(y_test, y_pred, sensitive_features=A_test)]\n",
    "#         })\n",
    "        \n",
    "#         fairlearn_data.append(fairlearn_metrics)\n",
    "    \n",
    "#     # 모든 결과를 하나의 DataFrame으로 결합\n",
    "#     fairlearn = pd.concat(fairlearn_data, ignore_index=True)\n",
    "#     # 설명 문자열 생성\n",
    "#     explanation = (\"\\ndemographic_partiy_difference(인구 동등성 차이) : 머신러닝 예측 확률이 민감 집단으로 인해 영향받지 않는 정도의 차이. 0에 가까울수록 적당\\n\" \n",
    "#                    \"demographic_partiy_ratio(인구 동등성 비율) : 머신러닝 예측 확률이 민감 집단으로 인해 영향받지 않는 정도의 차이. 1에 가까울수록 적당\\n\"\n",
    "#                    \"equalized_odds_difference(균등화된 확률 차이) : 예측 정도가 민감 집단에 영향받지 않는 정도는 물론, FP와 TP가 같은 수치를 보이고 있음을 나타내는 정도이다. 추가로 FP란 실제값이 거짓인데 참이라고 예측한 확률이며, TP는 실제값이 참이고, 참이라고 예측한 확률을 의미한다. 0에 가까울수록 모델이 더 공정. 0에서 멀어질수록 편향적.\\n\"\n",
    "#                    \"equalized_odds_ratio(균등 기회 확률) : 머신러닝 모델의 예측 정확도가 서로 다른 민감 집단 간에 얼마나 일관되는지를 나타내는 비율. 1에 가까울수록 모델이 더 공정하다고 판단. 1에서 크게 벗어난 값은 특정 집단에 대한 편향이 존재한다고 추정 가능\")\n",
    "          \n",
    "#     # JSON으로 변환하여 저장\n",
    "#     convert_fairlearn_to_json(fairlearn, dataset)\n",
    "#     with open(f'json/{dataset}_fairlearn.json', 'r') as file:\n",
    "#         json_data = file.read()\n",
    "#     json_api(project,'fairlearn',json_data)\n",
    "#     return fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4704270d-fe03-4269-995a-779fd0fdb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_images_as_text_files(directory):\n",
    "#     if not os.path.exists(directory):\n",
    "#         raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n",
    "\n",
    "#     for filename in enumerate(os.listdir(directory)):\n",
    "#         if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "#             img_path = os.path.join(directory, filename)\n",
    "            \n",
    "#             # 이미지 파일 읽기\n",
    "#             with open(img_path, 'rb') as img_file:\n",
    "#                 image_bytes = img_file.read()\n",
    "#                 encoded_string = base64.b64encode(image_bytes).decode('utf-8')\n",
    "#                 image_data = encoded_string\n",
    "#                 print(f\"Encoded string for {filename}:\")\n",
    "#                  # 각 이미지 출력 사이에 빈 줄 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f30a9d2-f9f2-42c3-b073-68213ffe7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import base64\n",
    "# import requests\n",
    "\n",
    "# def autoviz_image_upload(pid, base_directory, dataset):\n",
    "#     if not os.path.exists(base_directory):\n",
    "#         raise FileNotFoundError(f\"Base directory '{base_directory}' does not exist.\")\n",
    "    \n",
    "#     image_url = 'http://192.168.10.127:8000/postdb/light/insertFile/image/'\n",
    "    \n",
    "#     # 테이블 이름과 해당하는 이미지 파일 이름을 매핑\n",
    "#     table_image_map = {\n",
    "#         \"before\": {\n",
    "#             \"Scatter_Plots\": \"Scatter_Plots.png\",\n",
    "#             \"Pair_Scatter_Plots\": \"Pair_Scatter_Plots.png\",\n",
    "#             \"Heat_Maps\": \"Heat_Maps.png\",\n",
    "#             \"Dist_Plots_target\": \"Dist_Plots_target.png\",\n",
    "#             \"Dist_Plots_Numerics\": \"Dist_Plots_Numerics.png\",\n",
    "#             \"Box_Plots\": \"Box_Plots.png\",\n",
    "#             \"Bar_Plots\": \"Bar_Plots.png\"\n",
    "#         },\n",
    "#         \"after\": {\n",
    "#             \"Scatter_Plots_after\": \"Scatter_Plots.png\",\n",
    "#             \"Pair_Scatter_Plots_after\": \"Pair_Scatter_Plots.png\",\n",
    "#             \"Heat_Maps_after\": \"Heat_Maps.png\",\n",
    "#             \"Dist_Plots_target_after\": \"Dist_Plots_target.png\",\n",
    "#             \"Dist_Plots_Numerics_after\": \"Dist_Plots_Numerics.png\",\n",
    "#             \"Box_Plots_after\": \"Box_Plots.png\"\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     for directory in [\"before\", \"after\"]:\n",
    "#         dir_path = os.path.join(base_directory, directory, \"income\")\n",
    "#         if not os.path.exists(dir_path):\n",
    "#             print(f\"Warning: Directory '{dir_path}' does not exist. Skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         for table_name, image_file in table_image_map[directory].items():\n",
    "#             img_path = os.path.join(dir_path, image_file)\n",
    "            \n",
    "#             if not os.path.exists(img_path):\n",
    "#                 print(f\"Warning: File {img_path} not found. Skipping...\")\n",
    "#                 continue\n",
    "            \n",
    "#             try:\n",
    "#                 with open(img_path, 'rb') as img_file:\n",
    "#                     image_bytes = img_file.read()\n",
    "#                     encoded_string = base64.b64encode(image_bytes).decode('utf-8')\n",
    "                \n",
    "#                 print(f\"Processing {image_file}...\")\n",
    "                \n",
    "#                 # API 요청 데이터 준비\n",
    "#                 image_postVal = {\n",
    "#                     'val1': pid,  # 프로젝트 ID\n",
    "#                     'val2': table_name,  # 테이블명\n",
    "#                     'val3': encoded_string  # 텍스트로 변환된 이미지\n",
    "#                 }\n",
    "                \n",
    "#                  # API 요청 보내기\n",
    "#                 response = requests.post(image_url, json=image_postVal)\n",
    "                \n",
    "#                 print(f\"Response status code: {response.status_code}\")\n",
    "#                 print(f\"Response content: {response.text}\")\n",
    "                \n",
    "#                 if response.status_code == 200:\n",
    "#                     try:\n",
    "#                         data = response.json()\n",
    "#                         if data is None:\n",
    "#                             print(f\"Warning: Received None response for {table_name}\")\n",
    "#                         else:\n",
    "#                             print(f\"Success for {table_name}: {data}\")\n",
    "#                     except json.JSONDecodeError:\n",
    "#                         print(f\"Warning: Unable to parse JSON response for {table_name}\")\n",
    "#                         print(f\"Raw response: {response.text}\")\n",
    "#                 else:\n",
    "#                     print(f'Error for {table_name}: {response.status_code}')\n",
    "#                     print(f'Error response: {response.text}')\n",
    "            \n",
    "#             except FileNotFoundError:\n",
    "#                 print(f\"Error: File {img_path} not found.\")\n",
    "#             except requests.exceptions.RequestException as e:\n",
    "#                 print(f\"API request failed for {table_name}: {e}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Unexpected error occurred while processing {image_file}: {e}\")\n",
    "# # 사용 예:\n",
    "# # autoviz_image_upload(\"project_id\", \"/path/to/base/directory\", \"dataset_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
